# Next Word Predictor Using LSTM

This project implements a **Next Word Predictor** using a Long Short-Term Memory (LSTM) model, a type of Recurrent Neural Network (RNN) architecture. The model predicts the next word in a given sequence of words, making it useful for applications like text autocompletion or predictive text input.

## Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Preprocessing](#preprocessing)
- [Training](#training)
- [Evaluation](#evaluation)
- [How to Use](#how-to-use)
- [Requirements](#requirements)

## Overview

This project demonstrates the usage of LSTMs to generate the next probable word based on a given input sequence. LSTMs are particularly effective in sequence prediction tasks due to their ability to capture long-term dependencies in data.

## Dataset

A text corpus is used to train the model. Example datasets:
- **Gutenberg Corpus**: A collection of books available in the public domain.
- **Custom Dataset**: Any text data can be used, such as chat logs, articles, or literature.

### Dataset Processing
1. Tokenized the text data into words.
2. Created sequences of fixed lengths to serve as input for the LSTM model.
3. Converted words into numerical representations using a word index (via `Tokenizer` in Keras).

## Model Architecture

The LSTM-based model includes:
- **Embedding Layer**: Converts words into dense vectors of fixed size.
- **LSTM Layer**: Captures the temporal relationships between words in sequences.
- **Dense Layer**: Outputs a probability distribution over the vocabulary.

### Summary of the Model:
- Input: Sequence of words of fixed length (`n` words).
- Output: Predicted next word.

```python
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_len-1),
    LSTM(units=100),
    Dense(units=vocab_size, activation='softmax')
])
```

## Preprocessing

1. **Text Cleaning**: Removed punctuation, special characters, and converted text to lowercase.
2. **Tokenization**:
   - Used Keras `Tokenizer` to convert text into sequences of integers.
   - Generated word-to-index and index-to-word mappings.
3. **Sequence Preparation**:
   - Split data into input (`X`) and output (`y`) sequences.
   - Input: First `n` words in a sequence.
   - Output: The `n+1`th word.
4. **Padding**: Ensured all sequences have the same length using `pad_sequences`.

## Training

The model was trained using:
- **Loss Function**: Categorical Cross-Entropy.
- **Optimizer**: Adam.
- **Batch Size**: 64.
- **Epochs**: 20 (configurable).

```python
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X, y, epochs=20, batch_size=64, verbose=1)
```

## Evaluation

The model was evaluated using:
- **Perplexity**: A measure of how well the model predicts sequences.
- **Accuracy**: On a separate validation dataset.

Visualization of training loss and accuracy over epochs:

```python
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['loss'])
plt.title('Model Accuracy and Loss')
plt.xlabel('Epoch')
plt.legend(['Accuracy', 'Loss'])
plt.show()
```

## How to Use

1. Clone this repository:
    ```bash
    git clone https://github.com/yourusername/next-word-predictor.git
    cd next-word-predictor
    ```

2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Train the model on your dataset:
    ```python
    python train.py
    ```

4. Use the model to predict the next word:
    ```python
    python predict.py
    ```

Example usage in Python:
```python
input_text = "I am learning"
predicted_word = predict_next_word(input_text, model, tokenizer)
print(f"Next word: {predicted_word}")
```

## Requirements

- Python >= 3.7
- TensorFlow >= 2.0
- Numpy
- Matplotlib
- Keras
- NLTK

Install dependencies with:
```bash
pip install tensorflow numpy matplotlib nltk
```

## Future Improvements

- Train the model on larger datasets like Wikipedia or Common Crawl for better generalization.
- Use pre-trained embeddings like GloVe or Word2Vec for the embedding layer.
- Implement beam search for generating more accurate predictions.
