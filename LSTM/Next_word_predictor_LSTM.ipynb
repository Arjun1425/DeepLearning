{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:22:58.824529Z",
     "start_time": "2024-12-19T00:22:58.710095Z"
    }
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re \n",
    "\n",
    "def extract_sentences_from_pdf(pdf_file):\n",
    "    doc = fitz.open(pdf_file)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in doc[5:]:\n",
    "        text += page.get_text()\n",
    "        \n",
    "    #Cleaning the text\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z\\s.]\", \"\", text)\n",
    "    # Remove extra spaces, including those before periods\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s\\.', '.', cleaned_text)\n",
    "    \n",
    "    doc.close()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentences = extract_sentences_from_pdf(\"./Arjun_Thesis_Proposal.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:22:58.923901Z",
     "start_time": "2024-12-19T00:22:58.825968Z"
    }
   },
   "id": "ecc763fb9df99255",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER INTRODUCTION AND BACKGROUND. Biomolecular condensates Biomolecular condensates found in biology commonly function as membraneless organelles that form through liquidliquid phase separation LLPS a process where proteins and nucleic acids as semble into dynamic highlyconcentrated and spatiallysegregated dropletlike structures within cells Figure.. Figure. A cartoon showing a cell with different compartments condensates that organize biomolecules involved in similar functions. The primary \n"
     ]
    }
   ],
   "source": [
    "print(sentences[:500])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:22:58.927567Z",
     "start_time": "2024-12-19T00:22:58.924547Z"
    }
   },
   "id": "3ef1587300b0587c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:10.937117Z",
     "start_time": "2024-12-19T00:23:10.930307Z"
    }
   },
   "id": "9fa714913be2b376",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:03.956185Z",
     "start_time": "2024-12-19T00:23:03.954365Z"
    }
   },
   "id": "29932acc1c541d26",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens/unique words: 1930\n"
     ]
    }
   ],
   "source": [
    "# Giving number to every unique word in the data. \n",
    "tokenizer.fit_on_texts([sentences])\n",
    "print(f\"Total number of tokens/unique words: {len(tokenizer.word_index)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:03.965238Z",
     "start_time": "2024-12-19T00:23:03.957401Z"
    }
   },
   "id": "91f4db331afc4ff0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('of', 3), ('to', 4), ('a', 5), ('in', 6), ('phase', 7), ('for', 8), ('we', 9), ('sequence', 10)]\n"
     ]
    }
   ],
   "source": [
    "lis = list(tokenizer.word_index.items())[:10]\n",
    "print(lis)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:03.969585Z",
     "start_time": "2024-12-19T00:23:03.966369Z"
    }
   },
   "id": "639b78a0c2f93e55",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Break data into sentences. \n",
    "# After that make input sequence dataset. \n",
    "input_sequences = []\n",
    "for sentence in sentences.split('.'):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    \n",
    "    for i in range(1, len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i + 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:03.981012Z",
     "start_time": "2024-12-19T00:23:03.970781Z"
    }
   },
   "id": "201804cc0588193a",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "print(tokenizer.texts_to_sequences([sentence])[0])\n",
    "--> \n",
    "[271, 878, 2, 879]\n",
    "[36, 15, 36, 15, 407, 6, 163, 562, 79, 13, 272, 273, 21, 118, 332, 226, 7, 14, 164, 5, 165, 95, 17, 2, 563, 109, 13, 880, 26, 564, 881, 2, 882, 883, 227, 59, 408, 22]\n",
    "[]\n",
    "[22]\n",
    "[5, 884, 409, 5, 190, 16, 145, 565, 15, 21, 410, 228, 566, 6, 274, 333]\n",
    "[1, 885, 334, 275, 43, 101, 411, 335, 886, 887, 276, 91, 13, 888, 567, 412, 568, 569, 889, 2, 890, 19, 32, 891, 892, 2, 893, 894, 895, 19, 32, 17, 2, 570, 229, 336, 896, 897, 62, 44, 166, 277]\n",
    "[27, 19, 278, 4, 1, 8, 898, 3, 571, 572, 45, 573, 22]............."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a95454a7439eff32"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of the sequence is 53\n",
      "[[   0    0    0 ...    0  271  878]\n",
      " [   0    0    0 ...  271  878    2]\n",
      " [   0    0    0 ...  878    2  879]\n",
      " ...\n",
      " [   0    0    0 ...   43 1930  129]\n",
      " [   0    0    0 ... 1930  129  330]\n",
      " [   0    0    0 ...  129  330   47]]\n"
     ]
    }
   ],
   "source": [
    "# Using the zero padding to make every sequence length same. \n",
    "max_length = max([len(x) for x in input_sequences])\n",
    "print(f\"Max length of the sequence is {max_length}\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre')\n",
    "print(padded_input_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:03.996582Z",
     "start_time": "2024-12-19T00:23:03.981733Z"
    }
   },
   "id": "6c8d044f9fed6a73",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7832, 52)\n",
      "(7832,)\n"
     ]
    }
   ],
   "source": [
    "X = padded_input_sequences[ : , : -1]\n",
    "Y = padded_input_sequences[ : , -1]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:04.002619Z",
     "start_time": "2024-12-19T00:23:03.999935Z"
    }
   },
   "id": "829d47951de96afc",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7832, 1931)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# To convert output into categorical data. \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(Y, num_classes= len(tokenizer.word_index) + 1)     # Added one because one hot encoding start indexing with zero and tokenizer start with one.\n",
    "print(Y.shape)\n",
    "print(Y[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:04.014887Z",
     "start_time": "2024-12-19T00:23:04.003943Z"
    }
   },
   "id": "b3c449262bd76dd9",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "NN Architecture. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8dc69baf3f55971"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T00:23:04.017611Z",
     "start_time": "2024-12-19T00:23:04.015958Z"
    }
   },
   "id": "c5c00b10d86c21e1",
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
